{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40923d3-27a6-49ad-a8a1-a901bd085572",
   "metadata": {},
   "source": [
    "### Objectif: IA Generative Sans API\n",
    "\n",
    "#### Créer un assistant capable de :\n",
    "\n",
    "    Comprendre et répondre à des questions sur une image (ex. \"Qu'est-ce que cette photo montre ?\").\n",
    "    Générer des images à partir de descriptions textuelles (ex. \"Dessine un chat jouant sur la lune\").\n",
    "    Dialoguer en langage naturel pour fournir des réponses textuelles ou multimodales.\n",
    "\n",
    "#### Architecture du Chatbot Multimodal\n",
    "\n",
    "    Module Vision :\n",
    "        Identifier et comprendre le contenu visuel d’une image.\n",
    "        Utiliser un modèle comme CLIP ou BLIP pour le traitement des images.\n",
    "\n",
    "    Module Langage :\n",
    "        Gérer les dialogues et répondre aux requêtes.\n",
    "        Utiliser un LLM comme GPT.\n",
    "\n",
    "    Module Génération d’Images :\n",
    "        Créer des images basées sur les descriptions.\n",
    "        Modèles recommandés : Stable Diffusion ou DALL·E.\n",
    "\n",
    "    Intégration :\n",
    "        Fusionner ces modules pour créer une interface utilisateur (UI), par exemple dans une application web avec Gradio ou Streamlit.\n",
    "\n",
    "#### Étapes de mise en œuvre\n",
    "1. Installer les dépendances\n",
    "\n",
    "Vous aurez besoin des bibliothèques suivantes :\n",
    "\n",
    "    Hugging Face Transformers : Pour les modèles de vision et de langage.\n",
    "    diffusers : Pour la génération d'images.\n",
    "    Gradio : Pour créer une interface utilisateur simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39cfaef-2b9e-476c-b54a-c544a9987237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ce199d3-fdb8-44f3-975c-9adcaadf924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers diffusers gradio torch pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e77df1e-a4ef-435c-adc7-2771bc8dfd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a480afc7-31c1-48ca-9c9c-ca1951bf0960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de l'appareil (CPU ou GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" #si vous avez un pc GPU ou CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffec749-f8ba-408e-8353-0dd1a62cc0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b500501-fae2-4ab5-af3d-2bc55b7d3149",
   "metadata": {},
   "source": [
    "### Intégrer les fonctionnalités\n",
    "\n",
    "Crée une interface utilisateur pour interagir avec ces modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198a8fd4-8aad-4ef1-8510-749633454523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41933fc0-ed9a-4aaa-986e-fda152c871ec",
   "metadata": {},
   "source": [
    "### Utiliser un modèle valide pour le français\n",
    "\n",
    "Il y a plusieurs modèles disponibles sur Hugging Face qui fonctionnent bien pour le français. Par exemple :\n",
    "\n",
    "    Flan-T5 (multilingue)\n",
    "    GPT-J-6B (multi-langues)\n",
    "    Bloom (spécifiquement entraîné pour le français et d'autres langues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54b5487-8e90-462e-813b-9698960a9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc0d52-871f-4c94-abe9-96fa0cbc710d",
   "metadata": {},
   "source": [
    "#### On va charger les differents models: pour l'interpretation d'image et la génération d'imgae, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48b9e0f-ed7a-4654-bc63-1f48e71232ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580d5d735a1945838ff8b2679a3825fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3757f9119344d38b284e78b2d000f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7fe73486b441d89ed952e2b430759e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c9bff440c046738698c3fe907a2e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec41b24b42274b489b1c7c2ea7ac7413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2dd7975c2a47d9930972a5c7aa6623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05df1dcb4b76445680d62989e8090143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Charger les modèles\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Utiliser un modèle local pour la génération de texte (GPT-Neo pour de meilleures performances)\n",
    "#text_generator_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\").to(device)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "\n",
    "#Pour francais\n",
    "#text_generator_model = AutoModelForCausalLM.from_pretrained(\"camenduru/french-gpt-j\").to(device)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"camenduru/french-gpt-j\")\n",
    "\n",
    "#model_name = \"bigscience/bloom-560m\"\n",
    "#text_generator_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Charger le modèle Flan-T5\n",
    "model_name = \"google/flan-t5-small\"\n",
    "text_generator_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Charger Stable Diffusion pour la génération d'images\n",
    "stable_diffusion = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fonction : légender une image\n",
    "def caption_image(image):#en general: processor, generate, decode\n",
    "    try:\n",
    "        inputs = blip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        output = blip_model.generate(**inputs)\n",
    "        return blip_processor.decode(output[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        return f\"Erreur lors de la légende de l'image : {e}\"\n",
    "\n",
    "# Fonction : générer du texte\n",
    "def generate_text(prompt):\n",
    "    try:\n",
    "        prompt = f\"Répondez de manière claire et concise à la question suivante : {prompt}\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = text_generator_model.generate(inputs.input_ids, max_length=100, num_return_sequences=1, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        return f\"Erreur lors de la génération de texte : {e}\"\n",
    "\n",
    "# Fonction : générer une image\n",
    "def create_image(prompt):\n",
    "    try: #pour gerer les erreur\n",
    "        image = stable_diffusion(prompt).images[0]\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        return f\"Erreur lors de la génération de l'image : {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b5830-00ec-44d3-8fd4-3e8dc1862334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b4b0348-1603-4dfd-9b28-6efa68ec2168",
   "metadata": {},
   "source": [
    "### Interface utilisateur Gradio\n",
    "\n",
    "Ici on va creer notre interface User, où on peut interagir avec notre IA. On le fera avec Gradio. vous ouvez aussi le faire avec Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a19322f-35be-4acf-a414-ac68c06a020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interface utilisateur Gradio\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Chatbot Multimodal Sans API : Interprétation et Création\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            user_input = gr.Textbox(label=\"Texte ou question\", placeholder=\"Décris une image ou pose une question...\")\n",
    "            image_input = gr.Image(label=\"Image (optionnel)\", type=\"pil\")\n",
    "            submit_button = gr.Button(\"Soumettre\")\n",
    "        with gr.Column():\n",
    "            text_output = gr.Textbox(label=\"Réponse\")\n",
    "            image_output = gr.Image(label=\"Image générée (si applicable)\")\n",
    "\n",
    "    # Fonction associée au bouton\n",
    "    def process_input(user_input, image_input):\n",
    "        if user_input.lower().startswith((\"dessine\", \"crée\", \"génère\")):\n",
    "            generated_image = create_image(user_input)\n",
    "            return \"\", generated_image\n",
    "        elif image_input is not None:\n",
    "            caption = caption_image(image_input)\n",
    "            response = f\"Cette image montre : {caption}\"\n",
    "            return response, None\n",
    "        else:\n",
    "            response = generate_text(user_input)\n",
    "            return response, None\n",
    "\n",
    "    submit_button.click(process_input, inputs=[user_input, image_input], outputs=[text_output, image_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21b3f5a0-a6f9-4dbc-bf39-3e07a143e776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380955f1cdb647d68913590c7e23cbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efiom\\anaconda3\\envs\\my_IAenv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\efiom\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-neo-125M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfc2b403ee544adb16f043dc272491e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf3e7f73b594db7b6c9e830d8544e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14808122dfdf47658a4a23eac42f260c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e708a5bc6814bec949168c7b3386394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe35040d7f14991b4cc100ff92e66fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6f2df2f8594467823e314b3a756c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ecb93abba344b6a152d46da037ac37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6808e94a814e2894c8a330e9c65e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lancer l'application\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e44d4fb-89ba-4bb5-bf6c-0bef005a0554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "* Running on public URL: https://93f3c6f9eba6ccd6e1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://93f3c6f9eba6ccd6e1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efiom\\anaconda3\\envs\\my_IAenv\\lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\efiom\\anaconda3\\envs\\my_IAenv\\lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\efiom\\anaconda3\\envs\\my_IAenv\\lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\efiom\\anaconda3\\envs\\my_IAenv\\lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Lancer l'application\n",
    "demo.launch(share=True)#share=True pour rendre public. sans cela ca va s'executer mais en local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92971b77-8d36-4ab5-9f34-c9dc05925e11",
   "metadata": {},
   "source": [
    "## Points clés de cette version :\n",
    "\n",
    " ####   Légende d'image :\n",
    " ####   Utilise BLIP pour analyser et interpréter les images localement.\n",
    "\n",
    "        \n",
    " ####  Génération de texte :\n",
    " ####  Utilise un modèle GPT-Neo local pour générer des réponses basées sur un prompt texte.\n",
    "\n",
    "        \n",
    " #### Génération d'images :\n",
    " #### Utilise Stable Diffusion, qui fonctionne localement pour créer des images basées sur un texte descriptif.\n",
    "\n",
    "        \n",
    " #### Interface conviviale :\n",
    " #### Via Gradio, tu peux fournir un texte ou une image et obtenir des résultats immédiats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48254bb7-dda4-4f26-bc9e-a3e40ab39e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (my_IAenv)",
   "language": "python",
   "name": "my_iaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
